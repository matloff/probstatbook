\chapter{Introduction to Confidence Intervals} 
\label{chap:confints} 

The idea of a confidence interval is central to statistical inference.
But actually, you already know about it---from the term {\it margin of
error} in news reports about opinion polls.
 
\section{The ``Margin of Error'' and Confidence Intervals}
\label{ciintro}

To explain the idea of margin of error, let's begin with a problem that
has gone unanswered so far:

In our simulations in previous units, it was never quite clear how long
the simulation should be run, i.e. what value to set for {\bf nreps} in
Section \ref{alohasim}.  Now we will finally address this issue.

As our example, consider the Bus Paradox, which presented in
Section \ref{busparadox}:  Buses arrive at a certain bus stop at random
times, with interarrival times being independent exponentially
distributed random variables with mean 10 minutes.  You arrive at the
bus stop every day at a certain time, say four hours (240 minutes) after
the buses start their morning run.  What is your mean wait $\mu$ for the
next bus?  

We found mathematically that, due to the memoryless property of the
exponential distribution, our wait is again exponentially distributed
with mean 10.  But suppose we didn't know that, and we wished to find
the answer via simulation.  (Note to reader:  Keep in mind throughout 
this example that we will be pretending that we don't know the 
mean wait is actually 10.  Reminders of this will be brought up
occasionally.)

We could write a program to do this:

\begin{Verbatim}[fontsize=\relsize{-2},numbers=left]
doexpt <- function(opt) {
   lastarrival <- 0.0
   while (lastarrival < opt) 
      lastarrival <- lastarrival + rexp(1,0.1)
   return(lastarrival-opt)
}

observationpt <- 240
nreps <- 1000
waits <- vector(length=nreps)
for (rep in 1:nreps) waits[rep] <- doexpt(observationpt)
cat("approx. mean wait = ",mean(waits),"\n")
\end{Verbatim}

Running the program yields

\begin{Verbatim}[fontsize=\relsize{-2}]
approx. mean wait = 9.653743
\end{Verbatim}

Note that $\mu$ is a population mean, where our ``population'' here is
the set of all possible bus wait times (some more frequent than others).
Our simulation, then, drew a sample of size 1000 from that population.
The expression {\bf mean(waits)} was our sample mean.

Now, was 1000 iterations enough?  How close is this value 9.653743 to
the true expected value of waiting time?\footnote{Of course, continue to
ignore the fact that we know that this value is 10.0.  What we're trying
to do here is figure out how to answer ``how close is it'' questions 
in general, when we don't know the true mean.}

What we would like to do is something like what the pollsters do during
presidential elections, when they say ``Ms. X is supported by 62\% of
the voters, with a margin of error of 4\%.''  In other words, we want to
be able to attach a margin of error to that figure of 9.653743 above.
We do this in the next section.

\section{Confidence Intervals for Means}
\label{cim}

We are now set to make use of the infrastructure that we've built up in
the preceding sections of this chapter.  Everything will hinge on
understanding that the sample mean is a random variable, with a known
approximate distribution.

{\bf The goal of this section (and several that follow) is to develop a
notion of margin of error, just as you see in the election campaign
polls.}  This raises two questions:

\begin{itemize}

\item [(a)] What do we mean by ``margin of error''?

\item [(b)] How can we calculate it?

\end{itemize}

% The answer to (a) is that we would like to make a statement, e.g. in the
% simulation output above, like ``We estimate the mean wait to be 9.65,
% and we are 95\% confident that the true population mean wait is between
% 9.65-0.22 and 9.65+0.22.''
% 
% Our answer to (b) will now be developed.

\subsection{Basic Formulation}
\label{ourfirstci}

So, suppose we have a random sample $W_1,...,W_n$ from some population
with mean $\mu$ and variance $\sigma^2$.

Recall that (\ref{firstz}) has an approximate N(0,1) distribution.  We
will be interested in the central 95\% of the distribution N(0,1).  Due
to symmetry, that distribution has 2.5\% of its area in the left tail
and 2.5\% in the right one.  Through the R call {\bf qnorm(0.025)}, or
by consulting a N(0,1) cdf table in a book, we find that the cutoff
points are at -1.96 and 1.96.  In other words, if some random variable
T has a N(0,1) distribution, then $P(-1.96 < T < 1.96) = 0.95$.

Thus

\begin{equation}
\label{cistart}
0.95 \approx P \left (-1.96 <  \frac{\overline{W}-\mu}{\sigma/\sqrt{n}} < 1.96 
\right )
\end{equation}

(Note the approximation sign.) Doing a bit of algebra on the
inequalities yields

\begin{equation}
\label{preci}
0.95 \approx P \left ( \overline{W} - 1.96 \frac{\sigma}{\sqrt{n}} < \mu
< \overline{W} + 1.96 \frac{\sigma}{\sqrt{n}} \right )
\end{equation}

Now remember, not only do we not know $\mu$, we also don't know
$\sigma$.  But we can estimate it, as we saw, via (\ref{s2}).  One can
show (the details will be given in Section \ref{slutsky}) that
(\ref{preci}) is still valid if we substitute $s$ for $\sigma$, i.e.

\begin{equation}
\label{theci}
0.95 \approx P \left ( \overline{W} - 1.96 \frac{s}{\sqrt{n}} < \mu
< \overline{W} + 1.96 \frac{s}{\sqrt{n}} \right ) 
\end{equation}

In other words, we are about 95\% sure that the interval 

\begin{equation}
\label{meanci}
(\overline{W} - 1.96 \frac{s}{\sqrt{n}}, \overline{W} + 1.96 \frac{s}{\sqrt{n}})
\end{equation}

contains $\mu$.  This is called a 95\% {\bf confidence interval} for
$\mu$.  The quantity $1.96 \frac{s}{\sqrt{n}}$ is the margin of error.

\subsection{Example:  Simulation Output}

We could add this feature to our program in Section \ref{ciintro}:

\label{bussim}
\begin{Verbatim}[fontsize=\relsize{-2},numbers=left]
doexpt <- function(opt) {
   lastarrival <- 0.0
   while (lastarrival < opt) 
      lastarrival <- lastarrival + rexp(1,0.1)
   return(lastarrival-opt)
}

observationpt <- 240
nreps <- 10000
waits <- vector(length=nreps)
for (rep in 1:nreps) waits[rep] <- doexpt(observationpt)
wbar <- mean(waits)
cat("approx. mean wait =",wbar,"\n")
s2 <- mean(waits^2) - wbar^2
s <- sqrt(s2)
radius <- 1.96*s/sqrt(nreps)
cat("approx. CI for EW =",wbar-radius,"to",wbar+radius,"\n")
\end{Verbatim}

When I ran this, I got 10.02565 for the estimate of EW, and got an
interval of (9.382715, 10.66859).  Note that the margin of error is the
radius of that interval, about 1.29/2.  We would then say, ``We are about
95\% confident that the true mean wait time is between 9.38 and 10.67.''

{\bf What does this really mean?}  This question is of the utmost
importance.  We will devote an entire section to it, Section
\ref{cimeaning}. 

Note that our analysis here is approximate, based on the Central Limit
Theorem, which was applicable because $\overline{W}$ involves a sum.  We
are making no assumption about the density of the population from which
the $W_i$ are drawn.  However, if that population density itself is
normal, then an exact confidence interval can be constructed.  This will
be discussed in Section \ref{studentt}.

\section{Meaning of Confidence Intervals}
\label{cimeaning}

\subsection{A Weight Survey in Davis}
\label{davisweights}

Consider the question of estimating the mean weight, denoted by $\mu$, of
all adults in the city of Davis.  Say we sample 1000 people at random,
and record their weights, with $W_i$ being the weight of the $i^{th}$
person in our sample.\footnote{Do you like our statistical pun here?
Typically an example like this would concern people's heights, not
weights.  But it would be nice to use the same letter for random
variables as in Section \ref{cim}, i.e. the letter W, so we'll have our
example involve people's weights instead of heights.  It works out
neatly, because the word {\it weight} has the same sound as {\it wait}.}

{\bf Now remember, we don't know the true value of that population mean,
$\mathbf{\mu}$---again, that's why we are collecting the sample
data, to estimate $\mathbf{\mu}$!  Our estimate will be our sample mean,
$\mathbf{\overline{W}}$.  But we don't know how accurate that estimate
might be.  That's the reason we form the confidence interval, as a gauge
of the accuracy of $\mathbf{\overline{W}}$ as an estimate of
$\mathbf{\mu}$.}

% \checkpoint

Say our interval (\ref{meanci}) turns out to be (142.6,158.8).  We say
that we are about 95\% confident that the mean weight $\mu$ of all
adults in Davis is contained in this interval.  {\bf What does this
mean?}  

Say we were to perform this experiment many, many times, recording the
results in a notebook:  We'd sample 1000 people at random, then record
our interval $(\overline{W} - 1.96 \frac{s}{\sqrt{n}}, \overline{W} + 1.96
\frac{s}{\sqrt{n}})$ on the first line of the notebook.  Then we'd
sample another 1000 people at random, and record what interval we got
that time on the second line of the notebook.  This would be a different
set of 1000 people (though possibly with some overlap), so we would get
a different value of $\overline{W}$ and so, thus a different interval; it
would have a different center and a different radius.  Then we'd do this
a third time, a fourth, a fifth and so on.  

Again, each line of the notebook would contain the information for a
different random sample of 1000 people.  There would be two columns for
the interval, one each for the lower and upper bounds.  And though it's
not immediately important here, note that there would also be columns
for $W_1$ through $W_{1000}$, the weights of our 1000 people, and
columns for $\overline{W}$ and s.

Now here is the point:  Approximately 95\% of all those intervals would
contain $\mu$, the mean weight in the entire adult population of Davis.
The value of $\mu$ would be unknown to us---once again, that's why we'd
be sampling 1000 people in the first place---but it does exist, and it
would be contained in approximately 95\% of the intervals.

As a variation on the notebook idea, think of what would happen if you
and 99 friends each do this experiment.  Each of you would sample 1000
people and form a confidence interval.  Since each of you would get a
different sample of people, you would each get a different confidence
interval.  What we mean when we say the confidence level is 95\% is that
of the 100 intervals formed---by you and 99 friends---about 95 of them
will contain the true population mean weight.  Of course, you hope you
yourself will be one of the 95 lucky ones!  But remember, you'll never
know whose intervals are correct and whose aren't.

% \checkpoint

{\bf Now remember, in practice we only take {\it one} sample of 1000
people.  Our notebook idea here is merely for the purpose of
understanding what we mean when we say that we are about 95\% confident
that one interval we form does contain the true value of $\mu$.}

% \subsection{Back to Our Bus Simulation}
% \label{backtobus}
% 
% Our simulation example in Section \ref{bus} is statistically the same as
% our Davis weights example.  Simulation is a sampling process.  If we
% simulate 1000 bus waits, as we did in that section, we are taking a
% random sample $W_1,...,W_{1000}$ of size 1000 from ``population'' of all
% bus waits.  In our program, we find the mean of our 1000 bus waits,
% which is our sample mean $\overline{W}$.  We are trying to estimate
% $\mu$, the mean bus wait in the population.  Again, this is not mere
% analogy; mathematically the two situations, weights and waits, are
% completely identical, two instances of the same principle.
% 
% Let's use the ``you and your 99 friends'' idea again.  Supposed each of
% you 100 people run the R program at the end of Section \ref{ourfirstci}.
% Each of you will get a different confidence interval printed out at the
% end of your run.\footnote{Recall that R will generate a different stream
% of random numbers each time you run your program, unless you call {\bf
% set.seed()}.}  Well, when we say that the program prints out a 95\%
% confidence interval, we mean that about 95 of you 100 people will have
% an interval that contains the true value of EW.
% 
% In the Davis weight example above, I stressed that we don't know $\mu$
% so as to estimate $\mu$!  But our bus simulation example is somewhat
% artificial, because we actually did know the value of $\mu$ here; it's 10.  
% In most simulations we wouldn't know $\mu$, but here we did, as there
% happened to be some ``bus theory'' available.  
% 
% But let's exploit the artificial nature of the bus example, as it will
% allow us to really see the ``you and 99 friends'' idea in action, as
% follows.
% 
% We'll expand the code to simulate 1000 people running the original
% program.  In other words, we'll add an extra outer loop to do 1000 runs
% of the program.  Each run will compute the confidence interval, and then
% we'll see in the end how many of the 1000 runs have a confidence
% interval that includes the true EW, 10.0:
% 
% \begin{Verbatim}[fontsize=\relsize{-2},numbers=left]
% doexpt <- function(opt) {
%    lastarrival <- 0.0
%    while (lastarrival < opt)
%       lastarrival <- lastarrival + rexp(1,0.1)
%    return(lastarrival-opt)
% }
% 
% observationpt <- 240
% nreps <- 1000
% numruns <- 1000
% waits <- vector(length=nreps)
% numcorrectcis <- 0  # number of conf. ints. that contain 10.0
% for (run in 1:numruns) {
%    for (rep in 1:nreps) waits[rep] <- doexpt(observationpt)
%    wbar <- mean(waits)
%    s2 <- mean(waits^2) - wbar^2
%    s <- sqrt(s2)
%    radius <- 1.96*s/sqrt(nreps)
%    if (abs(wbar - 10.0) <= radius) numcorrectcis <- numcorrectcis + 1
% }
% cat("approx. true confidence level=",numcorrectcis/numruns,"\n")
% \end{Verbatim}
% 
% In fact, the output of that program was 0.958, sure enough about 95\%.
% 
% Why is it not exactly 0.95?  
% 
% \begin{itemize}
% 
% \item We only simulated 1000 runs of the program; ideally it should 
% be an infinite number, to get the exact probability that an interval 
% contains $\mu$.
% 
% \item The Central Limit Theorem is only approximate.
% 
% \item Ideally we would use (\ref{preci}), but due to lack of knowledge
% of the true value of $\sigma$ (we don't know $\mu$, so why would we know
% $\sigma$?), we resorted to using s instead, in (\ref{meanci}).
% 
% \end{itemize}
% 
% {\bf Again remember that in practice we only do {\it one} run of
% simulating 1000 waits for the bus.  Our enhanced simulation code above
% is merely for the purpose of understanding what we mean when we say that
% we are about 95\% confident that one interval we form does contain the
% true value of $\mu$.}

% \checkpoint

\subsection{More About Interpretation}

Some statistics instructors give students the odd warning, ``You can't
say that the probability is 95\% that $\mu$ is IN the interval; you can
only say that the probability is 95\% confident that the interval
CONTAINS $\mu$.'' This of course is nonsense.  As any fool can see, the
following two statements are equivalent:

\begin{itemize}

\item ``$\mu$ is in the interval''

\item ``the interval contains $\mu$''

\end{itemize}

So it is ridiculous to say that the first is incorrect.  Yet many
instructors of statistics say so.

Where did this craziness come from?  Well, way back in the early days of
statistics, some instructor was afraid that a statement like ``The
probability is 95\% that $\mu$ is in the interval'' would make it sound
like $\mu$ is a random variable.  Granted, that was a legitimate fear,
because $\mu$ is not a random variable, and without proper warning, some
learners of statistics might think incorrectly.  The random entity is
the interval (both its center and radius), not $\mu$;  $\overline{W}$
and $s$ in (\ref{meanci}) vary from sample to sample, so the interval is
indeed the random object here, not $\mu$.  

So, it was reasonable for teachers to warn students not to think $\mu$
is a random variable.  But later on, some misguided instructor must have
then decided that it is incorrect to say ``$\mu$ is in the interval,''
and others then followed suit.  They continue to this day, sadly.

A variant on that silliness involves saying that one can't say ``The
probability is 95\% that $\mu$ is in the interval,'' because $\mu$ is
either in the interval or not, so that ``probability'' is either 1 or 0!
That is equally mushy thinking.

Suppose, for example, that I go into the next room and toss a coin,
letting it land on the floor.  I return to you, and tell you the coin is
lying on the floor in the next room.  I know the outcome but you don't.
What is the probability that the coin came up heads?  To me that is 1 or
0, yes, but to you it is 50\%, in any practical sense.  

It is also true in the ``notebook'' sense.  If I do this experiment many
times---go to the next room, toss the coin, come back to you,
go to the next room, toss the coin, come back to you, etc., one line of
the notebook per toss---then in the long run 50\% of the lines of the
notebook have Heads in the Outcome column.  

The same is true for confidence intervals.  Say we conduct many, many
samplings, one per line of the notebook, with a column labeled Interval
Contains Mu. Unfortunately, we ourselves don't get to see that column,
but it exists, and in the long run 95\% of the entries in the column
will be Yes.  

Finally, there are those who make a distinction between saying ``There
is a 95\% probability that...'' and ``We are 95\% confident that...''
That's silly too.  What else could ``95\% confident'' mean if not
95\% probability?

Consider the experiment of tossing two fair dice.  The probability is
34/36, or about 94\%, that we get a total that is different from 2 or
12.  As we toss the dice, what possible distinction could be made
between saying, ``The probability is 94\% that we will get a total between 3
and 11'' and saying, ``We are 94\% confident that we will get a total
between 3 and 11''?  The notebook interpretation supports both
phrasings, really.  The words {\it probability} and {\it confident}
should not be given much weight here; remember the quote at the
beginning of our Chapter 1:

\begin{quote}
{\it I learned very early the difference between knowing the name of
something and knowing something}---Richard Feynman, Nobel laureate in
physics
\end{quote}


\section{Confidence Intervals for Proportions}
\label{propcis}

So we know how to find confidence intervals for means.  How about
proportions?

\subsection{Derivation}
\label{derivation}

For example, in an election opinion poll, we might be interested in the
proportion p of people in the entire population who plan to vote for
candidate A.  

We will estimate p by taking a random sample of n voters, and finding
the {\it sample} proportion of voters who plan to vote for A.  The
latter is usually denoted $\widehat{p}$, pronounced ``p-hat.''  (The
symbol  $\widehat{ }$ is often used in statistics to mean ``estimate
of.'')

Assign to each voter in the population a value of Y, 1 if he/she plans
to vote for A, 0 otherwise.  Let $Y_i$ be the value of Y for the
i$^{th}$ person in our sample.  Then

\begin{equation}
\label{specialcaseofmean}
\widehat{p} = \overline{Y}
\end{equation}

where $\overline{Y}$ is the sample mean among the $Y_i$,
and p is the population mean of Y.

% It turns out that we already have our answer, from Section
% \ref{indicator}.  We found there that proportions are special cases of
% means:  If Y is an indicator random variable with P(Y = 1) = p,
% then EY = p.

So we are really working with means after all, and thus in order to get
a confidence interval for p from $\widehat{p}$, we can use
(\ref{meanci})!  We have that an approximate 95\% confidence interval
for p is

\begin{equation}
\label{tmppropci}
\left ( 
\widehat{p} 
- 1.96 s / \sqrt{n},
\widehat{p}
+ 1.96 s / \sqrt{n}
\right ) 
\end{equation}

where as before $s^2$ is the sample variance among the $Y_i$, defined in
\ref{s2}.

But there's more, because we can exploit the fact that in this special
case, each $Y_i$ is either 1 or 0, in order to save ourselves a bit of
computation, as follows:  

Recalling the convenient form of $s^2$, (\ref{alts2}), we have

\begin{eqnarray}
s^2 &=& 
\frac{1}{n} \sum_{i=1}^{n} Y_i^2 - \overline{Y}^2 \label{consistent} \\
&=& \frac{1}{n} \sum_{i=1}^{n} Y_i - \overline{Y}^2 \\
&=& \overline{Y} - \overline{Y}^2 \\
&=& \widehat{p} - \widehat{p}^2 
\end{eqnarray} 

Then (\ref{tmppropci}) simplifies to

\begin{equation}
\label{propci}
\left ( 
\widehat{p} 
- 1.96 \sqrt{\widehat{p} (1-\widehat{p})/n},
\widehat{p}
+ 1.96 \sqrt{\widehat{p} (1-\widehat{p})/n}
\right ) 
\end{equation}

\subsection{That n vs. n-1 Thing Again}
\label{nvsnminu1again}

Recall Section \ref{dividebywhat}, in which it was noted that this
book's definition of the sample variance, (\ref{s2}), is a little at
odds with the the way most books define it, (\ref{theirs2}).  The above
derivation sheds a bit more light on this topic.

In the way I've defined things here, I was consistent:  I divided by n
both in (\ref{s2}) and in (\ref{consistent}).  Yet most books divide by
n-1 in the former case but by n in the latter case!  Their version of
(\ref{propci}) is exactly the same as mine, yet they use a different $s$
in (\ref{meanci})---even though they too observe that the proportions
case is just a special case of estimating means (as in
(\ref{specialcaseofmean})).  So, another reason to divide by n in
(\ref{s2}) is to be consistent.

Again, the difference is usually minuscule anyway, but conceptually it's
important to understand.  As noted earlier, the n-1 divisor is really
just a historical accident.

\subsection{Simulation Example Again}

In our bus example above, suppose we also want our simulation to print
out the (estimated) probability that one must wait longer than 6.4
minutes.  As before, we'd also like a margin of error for the output.

We incorporate (\ref{propci}) into our program:

\begin{Verbatim}[fontsize=\relsize{-2},numbers=left]
doexpt <- function(opt) {
   lastarrival <- 0.0
   while (lastarrival < opt)
      lastarrival <- lastarrival + rexp(1,0.1)
   return(lastarrival-opt)
}

observationpt <- 240
nreps <- 1000
waits <- vector(length=nreps)
for (rep in 1:nreps) waits[rep] <- doexpt(observationpt)
wbar <- mean(waits)
cat("approx. mean wait =",wbar,"\n")
s2 <- (mean(waits^2) - mean(wbar)^2)
s <- sqrt(s2)
radius <- 1.96*s/sqrt(nreps)
cat("approx. CI for EW =",wbar-radius,"to",wbar+radius,"\n")
prop <- length(waits[waits > 6.4]) / nreps
s2 <- prop*(1-prop)
s <- sqrt(s2)
radius <- 1.96*s/sqrt(nreps)
cat("approx. P(W > 6.4) =",prop,", with a margin of error of",radius,"\n")
\end{Verbatim}

When I ran this, the value printed out for $\widehat{p}$ was 0.54,
with a margin of error of 0.03, thus an interval of (0.51,0.57).
We would say, ``We don't know the exact value of $P(W > 6.4)$, so we ran
a simulation.  The latter estimates this probability to be 0.54, with a
95\% margin of error of 0.03.''

\subsection{Example:  Davis Weights} 
\label{severalexamples}

Note again that this uses the same principles as our Davis weights
example.  Suppose we were interested in estimating the proportion of
adults in Davis who weigh more than 150 pounds.  Suppose that proportion
is 0.45 in our sample of 1000 people.  This would be our estimate
$\widehat{p}$ for the population proportion $p$, and an approximate 95\%
confidence interval (\ref{propci}) for the population proportion would be
(0.42,0.48).  We would then say, ``We are 95\% confident that the true
population proportion p of people who weigh over 150 pounds is between
0.42 and 0.48.''

Note also that although we've used the word {\it proportion} in the
Davis weights example instead of {\it probability}, they are the same.
If I choose an adult at random from the population, the probability that
his/her weight is more than 150 is equal to the proportion of adults in
the population who have weights of more than 150.

And the same principles are used in opinion polls during presidential
elections.  Here $p$ is the population proportion of people who plan to
vote for the given candidate.  This is an unknown quantity, which is
exactly the point of polling a sample of people---to estimate that
unknown quantity p.  Our estimate is $\widehat{p}$, the proportion of
people in our sample who plan to vote for the given candidate, and n is
the number of people that we poll.  We again use (\ref{propci}).

\subsection{Interpretation}

The same interpretation holds as before.  Consider the examples in the
last section:

\begin{itemize}

\item If each of you and 99 friends were to run the R program at the
beginning of Section \ref{severalexamples}, you 100 people would get 100
confidence intervals for $P(W > 6.4)$.  About 95 of you would have
intervals that do contain that number.

\item If each of you and 99 friends were to sample 1000 people in Davis
and come up with confidence intervals for the true population proportion
of people who weight more than 150 pounds, about 95 of you would have
intervals that do contain that true population proportion.

\item If each of you and 99 friends were to sample 1200 people in an
election campaign, to estimate the true population proportion of people
who will vote for candidate X, about 95 of you will have intervals that
do contain this population proportion.

\end{itemize}

Of course, this is just a ``thought experiment,'' whose goal is to
understand what the term ``95\% confident'' really means.  In practice,
we have just one sample and thus compute just one interval.  But we say
that the interval we compute has a 95\% chance of containing the
population value, since 95\% of all intervals will contain it. 

% \checkpoint

\subsection{(Non-)Effect of the Population Size}

Note that in both the Davis and election examples, it doesn't matter
what the size of the population is.  The approximate distribution of
$\widehat{p}$ is N(p,p(1-p)/n), so the accuracy of $\widehat{p}$, depends
only on $p$ and $n$.  So when people ask, ``How a presidential election
poll can get by with sampling only 1200 people, when there are more than
100,000,000 voters in the U.S.?'' now you know the answer.  (We'll
discuss the question ``Why 1200?'' below.)

Another way to see this is to think of a situation in which we wish to
estimate the probability p of heads for a certain coin.  We toss the
coin n times, and use $\widehat{p}$ as our estimate of p.  Here our
``population''---the population of all coin tosses---is infinite, yet it
is still the case that 1200 tosses would be enough to get a good
estimate of p.

\subsection{Inferring the Number Polled}

A news report tells us that in a poll, 54\% of those polled supported
Candidate A, with a 2.2\% margin of error.  Assuming that the methods
here were used, with a 95\% level of confidence, let's find the
approximate number polled.

\begin{equation}
0.022 = 1.96 \times \sqrt{0.54 \cdot 0.46 / n}
\end{equation}

Solving, we find that n is approximately 1972.

\subsection{Planning Ahead}

Now, why do the pollsters often sample 1200 people?  

First, note that the maximum possible value of $\widehat{p}
(1-\widehat{p})$ is 0.25.\footnote{Use calculus to find the maximum
value of f(x) = x(1-x).} Then the pollsters know that their margin of
error with n = 1200 will be at most $1.96 \times 0.5/\sqrt{1200}$, or
about 3\%, even before they poll anyone.  They consider 3\% to be
sufficiently accurate for their purposes, so 1200 is the n they choose.

\section{General Formation of Confidence Intervals from Approximately
Normal Estimators}
\label{stderrest}

We would now like to move on to constructing confidence intervals for
other settings than the case handled so far, estimation of a single
population mean or proportion.

\subsection{The Notion of a Standard Error}
\label{notionofse}

Suppose we are estimating some population quantity $\theta$ based on
sample data $Y_1,...,Y_n$.  So far, our only examples have had $\theta$
as a population mean $\mu$, a population proportion p.  But we'll see
other examples as things unfold in this and subsequent chapters.  

Consider an estimate for $\theta$, $\widehat{\theta}$, and suppose that
the estimator is composed of some sum for which the Central Limit
Theorem applies,\footnote{Using more advanced tools (Section
\ref{delta}), one can show approximate normality even in many nonlinear
cases.}, so that the approximate distribution of  $\widehat{\theta}$ 
is normal with mean $\theta$ and some variance.

Ponder this sequence of points: 

\begin{itemize}

\item $\widehat{\theta}$ is a random variable.

\item Thus $\widehat{\theta}$ has a variance.

\item Thus $\widehat{\theta}$ has a standard deviation $\eta$.

\item Unfortunately, $\eta$ is an unknown population quantity.

\item But we may be able to estimate $\eta$ from our sample data.  Call
that estimate $\widehat{\eta}$.

\item We refer to $\widehat{\eta}$ as the {\it standard error} of
$\widehat{\theta}$, or $\textrm{s.e.}\widehat{\theta}$.

\end{itemize}

Back in Section \ref{ourfirstci}, we found a standard error for
$\overline{W}$, the sample mean, using the following train of thought:

\begin{itemize}

\item $Var(\overline{W}) = \frac{\sigma^2}{n}$

\item $\widehat{Var}(\overline{W}) = \frac{s^2}{n}$

\item $\textrm{s.e.}(\overline{W}) = \frac{s}{\sqrt{n}}$

\end{itemize}

In many cases, deriving a standard error is more involved than the
above,  But the point is this:

\begin{quote}

Suppose $\widehat{\theta}$ is a sample-based estimator of a population
quantity $\theta$, and that, due to being composed of sums or some other
reason, $\widehat{\theta}$ is approximately normally distributed with
mean $\theta$, and some (possibly unknown) variance.  Then the quantity

\begin{equation}
\label{thetahatstderr}
\frac
{\widehat{\theta} - \theta}
{{\rm s.e.}(\widehat{\theta})}
\end{equation}

has an approximate N(0,1) distribution.
% \footnote{This also presumes that
% $\widehat{\theta}$ is a {\bf consistent} estimator of $\theta$, meaning
% that $\widehat{\theta}$ converges to $\theta$ as $n \rightarrow \infty$.
% There are some other technical issues at work here, but they are beyond
% the scope of this book.}

\end{quote}

\subsection{Forming General Confidence Intervals}

That means we can mimic the derivation that led to (\ref{meanci}).  As
with (\ref{cistart}), write

\begin{equation}
0.95 \approx P \left (-1.96 <  
\frac
{\widehat{\theta} - \theta}
{{\rm s.e.}(\widehat{\theta})}
< 1.96 \right )
\end{equation}

After going through steps analogous to those following (\ref{cistart}),
we find that an approximate 95\% confidence interval for $\theta$ is

\begin{equation}
\label{genci}
\widehat{\theta} \pm 1.96 \cdot {\rm s.e.}(\widehat{\theta})
\end{equation}

In other words, the margin of error is $1.96 \textrm{ s.e.}(\widehat{\theta})$.

{\bf The standard error of the estimate is one of the most commonly-used
quantities in statistical applications.  You will encounter it
frequently in the output of R, for instance, and in the subsequent
portions of this book.  Make sure you understand what it means and how
it is used.} 

And note again that $\sqrt{\widehat{p} (1-\widehat{p})/n}$ is the
standard error of $\widehat{p}$.

\subsection{Standard Errors of Combined Estimators}

Here is further chance to exercise your skills in the mailing tubes
regarding variance.

Suppose we have two population values to estimate, $\omega$ and
$\gamma$, and that we are also interested in the quantity $\omega + 2
\gamma$.  We'll estimate the latter with $\hat{\omega} + 2
\hat{\gamma}$.  Suppose the standard errors of $\hat{\omega}$ and
$\hat{\gamma}$ turn out to be 3.2 and 8.8, respectively, and that the
two estimators are independent and approximately
normal.\footnote{Technically, the term {\it standard error} is only used
for approximately normal estimators anyway.}
Let's find the standard error of
$\hat{\omega} + 2 \hat{\gamma}$.

We know from the material surrounding (\ref{lincombnormal}) that 
$\hat{\omega} + 2 \hat{\gamma}$ has an approximately normal distribution
with variance

\begin{equation}
Var(\hat{\omega}) + 2^2 Var(\hat{\gamma})
\end{equation}

% We have (make sure you can supply the reasons)
% 
% \begin{eqnarray}
% Var(\hat{\omega} + 2 \hat{\gamma}) &=& 
% Var(\hat{\omega}) + Var(2 \hat{\gamma}) \\ 
% &=& 
% \end{eqnarray}

Thus the standard error of $\hat{\omega} + 2 \hat{\gamma}$ is

\begin{equation}
\label{threetwo}
\sqrt{\cdot 3.2^2 + 2^2 \cdot 8.8^2}
\end{equation}

Now that we know the standard error of $\hat{\omega} + 2 \hat{\gamma}$,
we can use it in (\ref{genci}).  We add and subtract 1.96 times
(\ref{threetwo}) to $\hat{\omega} + 2 \hat{\gamma}$, and that is our
interval.

In general, for constants $a$ and $b$, an approximate 95\% confidence
interval for the population quantity $a \omega + b \gamma$ is

\begin{equation}
\label{gen2ci}
a \hat{\omega} + b \hat{\gamma} \pm 1.96 ~
\sqrt{a^2 s.e.^2(\hat{\omega}) +
      b^2 s.e.^2(\hat{\gamma})
}
\end{equation}

We can go even further.  If $\hat{\omega}$ and $\hat{\gamma}$ are not
independent but have known covariance, we can use the methods of Chapter
\ref{randvec} to obtain a standard error for any linear combination of
these two estimators.

\section{Confidence Intervals for Differences of Means or Proportions}
\label{diffs}

\subsection{Independent Samples}
\label{indepsams}

Suppose in our sampling of people in Davis we are mainly interested in
the difference in weights between men and women.  Let $\overline{X}$ and
$n_1$ denote the sample mean and sample size for men, and let $\overline{Y}$
and $n_2$ for the women.  Denote the population means and variances by
$\mu_i$ and $\sigma_i^2$, i = 1,2.  We wish to find a confidence
interval for $\mu_1 - \mu_2$.  The natural estimator for that quantity
is $\overline{X} - \overline{Y}$.  

So, how can we form a confidence interval for $\mu_1 - \mu_2$ using
$\overline{X} - \overline{Y}$?  Since the latter quantity is composed of
sums, we can use (\ref{genci}) and (\ref{gen2ci}).  Here:

\begin{itemize}

\item $a = 1, ~ b = -1$

\item $\omega = \mu_1, ~ \gamma = \mu_2$

\item $\hat{\omega} = \overline{X}, ~
       \hat{\gamma} = \overline{Y}$

\end{itemize}

\label{diffxbarybar}

But we know from before that $s.e.(\overline{X} = s_1 / \sqrt{n}$, 
where $s^2_1$ is the sample variance for the men, 

\begin{equation}
s_1^2 = \frac{1}{n_1}\sum_{i=1}^{n_1} (X_i - \overline{X})^2
\end{equation}

and similarly for $\overline{Y}$ and the women.  So, 
we have

\begin{equation}
\label{sediff}
\textrm{s.e.}(\overline{X} - \overline{Y}) =
\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
\end{equation}

Thus (\ref{genci}) tells us that an approximate 95\% confidence interval
for $\mu_1 - \mu_2$ is

\begin{equation}
\label{2indepsamp}
\left (\overline{X}-\overline{Y} - 1.96 \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}},
\overline{X}-\overline{Y} + 1.96 \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}} \right )
\end{equation}

% \checkpoint

What about confidence intervals for the difference in two population
proportions $p_1 - p_2$?  Recalling that in Section \ref{propcis} we
noted that proportions are special cases of means, we see that finding a
confidence interval for the difference in two proportions is covered by
(\ref{2indepsamp}).  Here

\begin{itemize}

\item $\overline{X}$ reduces to $\widehat{p}_1$

\item $\overline{Y}$ reduces to $\widehat{p}_2$

\item $s_1^2$ reduces to $\widehat{p}_1 (1 - \widehat{p}_1)$ 

\item $s_2^2$ reduces to $\widehat{p}_2 (1 - \widehat{p}_2)$

\end{itemize}

So, (\ref{2indepsamp}) reduces to

\begin{equation}
\label{2indepprop}
\widehat{p_1} - \widehat{p_2}  \pm R
\end{equation}

where the radius R is

\begin{equation}
1.96 ~ \sqrt{
   \frac{\widehat{p}_1 (1 - \widehat{p}_1)}{n_1} +
   \frac{\widehat{p}_2 (1 - \widehat{p}_2)}{n_2}
}
\end{equation}

\subsection{Example:  Network Security Application}
\label{netsecapp}

In a network security application, C. Mano {\it et
al}\footnote{RIPPS: Rogue Identifying Packet Payload Slicer Detecting
Unauthorized Wireless Hosts Through Network Traffic Conditioning, C.
Mano and a ton of other authors, {\sc ACM Transactions on Information
Systems and Security}, May 2007.} compare round-trip travel time for
packets involved in the same application in certain wired and wireless
networks.  The data was as follows:

\begin{tabular}{|l|l|l|l|}
\hline
sample & sample mean & sample s.d. & sample size \\ \hline  \hline
wired & 2.000 & 6.299 & 436 \\ \hline
wireless & 11.520 & 9.939 & 344 \\ \hline
\end{tabular}

We had observed quite a difference, 11.52 versus 2.00, but could it be
due to sampling variation?  Maybe we have unusual samples?  This calls
for a confidence interval!

Then a 95\% confidence interval for the difference between wireless and
wired networks is

\begin{equation}
11.520 - 2.000 \pm 1.96 
\sqrt{\frac{9.939^2}{344} + \frac{6.299^2}{436}}
= 9.52 \pm 1.22
\end{equation}

So you can see that there is a big difference between the two networks,
even after allowing for sampling variation.

\subsection{Dependent Samples}
\label{depsams}

Note carefully, though, that a key point above was the independence of
the two samples.  By contrast, suppose we wish, for instance, to find a
confidence interval for $\nu_1 - \nu_2$, the difference in mean heights
in Davis of 15-year-old and 10-year-old children, and suppose our data
consist of pairs of height measurements at the two ages on {\it the same
children}.  In other words, we have a sample of n children, and for the
$i^{th}$ child we have his/her height $U_i$ at age 15 and $V_i$ at age
10.  Let $\overline{U}$ and $\overline{V}$ denote the sample means.

The problem is that the two sample means are not independent.  If a
child is taller than his/her peers at age 15, he/she was probably taller
than them when they were all age 10.  In other words, for each i, $V_i$
and $U_i$ are positively correlated, and thus the same is true for
$\overline{V}$ and $\overline{U}$.  Thus we cannot use (\ref{2indepsamp}).

As always, it is instructive to consider this in ``notebook'' terms.
Suppose on one particular sample at age 10---one line of the
notebook---we just happen to have a lot of big kids.  Then
$\overline{V}$ is large.  Well, if we look at the same kids later at age
15, they're liable to be bigger than the average 15-year-old too.  In
other words, among the notebook lines in which $\overline{V}$ is large,
many of them will have $\overline{U}$ large too.

Since $\overline{U}$ is approximately normally distributed with mean
$\nu_1$, about half of the notebook lines will have $\overline{U} >
\nu_1$.  Similarly, about half of the notebook lines will have
$\overline{V} > \nu_2$.  But the nonindependence will be reflected in
MORE than one-fourth of the lines having both $\overline{U} > \nu_1$ and
$\overline{V} > \nu_2$.  (If the two sample means were 100\% correlated,
that fraction would be 1.0.)

Contrast that with a sample scheme in which we sample some 10-year-olds
and some 15-year-olds, say at the same time.  Now {\it there are
different kids in each of the two samples}.  So, if by happenstance we
get some big kids in the first sample, that has no impact on which kids
we get in the second sample.  In other words, $\overline{V}$ and
$\overline{U}$ will be independent.  In this case, one-fourth of the
lines will have both $\overline{U} > \nu_1$ and $\overline{V} > \nu_2$.

So, we cannot get a confidence interval for $\nu_1 - \nu_2$ from
(\ref{2indepsamp}), since the latter assumes that the two sample means
are independent.  What to do?

The key to the resolution of this problem is that the random variables
$T_i = V_i - U_i$, i = 1,2,...,n are still independent.  Thus we can use
(\ref{meanci}) on these values, so that our approximate 95\% confidence
interval is

\begin{equation}
(\overline{T} - 1.96 \frac{s}{\sqrt{n}}, \overline{T} + 1.96 \frac{s}{\sqrt{n}})
\end{equation}

where $\overline{T}$ and $s^2$ are the sample mean and sample variance
of the $T_i$.

A common situation in which we have dependent samples is that in which
we are comparing two dependent proportions.  Suppose for example that
there are three candidates running for a political office, A, B and C.
We poll 1,000 voters and ask whom they plan to vote for.  Let $p_A$,
$p_B$ and $p_C$ be the three population proportions of people planning
to vote for the various candidates, and let $\widehat{p}_A$,
$\widehat{p}_B$ and $\widehat{p}_C$ be the corresponding sample
proportions.  

Suppose we wish to form a confidence interval for $p_A - p_B$. Clearly,
the two sample proportions are not independent random variables, since
for instance if $\widehat{p}_A = 1$ then we know for sure that
$\widehat{p}_B$ is 0.  

Or to put it another way, define the indicator variables $U_i$ and $V_i$
as above, with for example $U_i$ being 1 or 0, according to whether the
$i^{th}$ person in our sample plans to vote for A or not, with $V_i$
being defined similarly for B.  Since $U_i$ and $V_i$ are
``measurements'' on {\it the same person}, they are not independent, and
thus $\widehat{p}_A$ and $\widehat{p}_B$ are not independent either.

Note by the way that while the two sample means in our kids' height
example above were positively correlated, in this voter poll example,
the two sample proportions are negatively correlated.

So, we cannot form a confidence interval for $p_A - p_B$ by using
(\ref{2indepprop}).  What can we do instead?

We'll use the fact that the vector $(N_A, N_B, N_C)^T$ has a multinomial
distribution, where $N_A$, $N_B$ and $N_C$ denote the numbers of people
in our sample who state they will vote for the various candidates (so
that for instance $\widehat{p}_A = N_A/1000$).  

Now to compute $Var(\widehat{p}_A - \widehat{p}_B)$, we make use of
(\ref{genvarxplusy}):

\begin{equation}
Var(\widehat{p}_A - \widehat{p}_B) =
Var(\widehat{p}_A) + 
Var(\widehat{p}_B) - 
2Cov(\widehat{p}_A, \widehat{p}_B)
\end{equation}

Or, we could have taken a matrix approach, using (\ref{covawaprime})
with A equal to the row vector (1,-1,0).

So, using (\ref{propcov}), the standard error of $\widehat{p}_A -
\widehat{p}_B$ is 

\begin{equation}
\label{seofdiffprops}
\sqrt{
0.001 \widehat{p}_A (1-\widehat{p}_A) +
0.001 \widehat{p}_B (1-\widehat{p}_B)
+0.002 \widehat{p}_A \widehat{p}_B
}
\end{equation}

\subsection{Example:  Machine Classification of Forest Covers}
\label{forestcover}

{\it Remote sensing} is machine classification of type from variables
observed aerially, typically by satellite.  The application we'll
consider here involves forest cover type for a given location; there
are seven different types.  (See Blackard, Jock A. and Denis J. Dean,
2000, ``Comparative Accuracies of Artificial Neural Networks and
Discriminant Analysis in Predicting Forest Cover Types from Cartographic
Variables,'' {\it Computers and Electronics in Agriculture},
24(3):131-151.)  Direct observation of the cover type is either too
expensive or may suffer from land access permission issues.  So, we wish
to guess cover type from other variables that we can more easily obtain.

One of the variables was the amount of hillside shade at noon, which
we'll call HS12.  {\it Here's our goal:}  Let $\mu_1$ and $\mu_2$ be the
population mean HS12 among sites having cover types 1 and 2,
respectively.  If $\mu_1 - \mu_2$ is large, then HS12 would be a good
predictor of whether the cover type is 1 or 2.  

So, we wish to estimate $\mu_1 - \mu_2$ from our data, in which we do
know cover type.  There were over 50,000 observations, but for
simplicity we'll just use the first 1,000 here.  Let's find an
approximate 95\% confidence interval for $\mu_1 - \mu_2$.  The two
sample means were 223.8 and 226.3, with s values of 15.3 and 14.3, and
the sample sizes were 226 and 585.  

Using (\ref{2indepsamp}), we have that the interval is

\begin{equation}
223.8 - 226.3 \pm 1.96 
\sqrt{\frac{15.3^2}{226} + \frac{14.3^2}{585}}
= -2.5 \pm 2.3
= (-4.8,-0.3)
\end{equation}

Given that HS12 values are in the 200 range (see the sample means), this
difference between them actually is not very large.  This is a great
illustration of an important principle, it will turn out in Section
\ref{whatswrong}.

As another illustration of confidence intervals, let's find one for the
difference in population proportions of sites that have cover types 1
and 2.  Our sample estimate is 

\begin{equation}
\widehat{p}_1 - \widehat{p}_2 = 0.226 - 0.585 = -0.359
\end{equation}

The standard error of this quantity, from (\ref{seofdiffprops}), is

\begin{equation}
\sqrt{
0.001 \cdot 0.226 \cdot 0.774
0.001 \cdot 0.585 \cdot 0.415
+002 \cdot 0.226 \cdot 0.585
} 
= 0.019
\end{equation}

That gives us a confidence interval of 

\begin{equation}
-0.359 \pm 1.96 \cdot 0.019 = (-0.397,-0.321)
\end{equation}

